<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>The Archives Unleashed Toolkit - The Archives Unleashed Project</title>
    <meta name="generator" content="Hugo 0.30.2" />

    
    <link rel="canonical" href="https://archivesunleashed.org/aut/">
    
    <meta name="author" content="The Archives Unleashed Project">
    

    <meta property="og:url" content="https://archivesunleashed.org/aut/">
    <meta property="og:title" content="The Archives Unleashed Project">
    <meta property="og:image" content="https://archivesunleashed.org/images/ArchivesUnleashedLogo.png">
    <meta name="apple-mobile-web-app-title" content="The Archives Unleashed Project">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://archivesunleashed.org/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://archivesunleashed.org/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://archivesunleashed.org/fonts/icon.eot');
        src: url('https://archivesunleashed.org/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://archivesunleashed.org/fonts/icon.woff')
               format('woff'),
             url('https://archivesunleashed.org/fonts/icon.ttf')
               format('truetype'),
             url('https://archivesunleashed.org/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }
    </style>

    <link rel="stylesheet" href="https://archivesunleashed.org/stylesheets/application.css">
    <link rel="stylesheet" href="https://archivesunleashed.org/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://archivesunleashed.org/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://archivesunleashed.org/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://archivesunleashed.org/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-grey palette-accent-red">




<div class="backdrop">
  <div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
  <nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        The Archives Unleashed Toolkit
      </div>
    </div>

    
    <div class="button button-twitter" role="button" aria-label="Twitter">
       <a href="https://twitter.com/unleasharchives" title="@unleasharchives on Twitter" target="_blank" class="toggle-button icon icon-twitter"></a>
    </div>
    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/archivesunleashed" title="@archivesunleashed on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
  <div class="drawer">
    <nav aria-label="Navigation">
  <a href="https://archivesunleashed.org/" class="project">
    <div class="banner">
      
        <div class="logo">
          <img src="https://archivesunleashed.org/images/ArchivesUnleashedLogo.png">
        </div>
      
      <div class="name">
        <strong>The Archives Unleashed Project </strong>
        
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Home" href="https://archivesunleashed.org/">
	
	Home
</a>



  
</li>



<li>
  
    



<a  title="About the Project" href="https://archivesunleashed.org/about-project/">
	
	About the Project
</a>



  
</li>



<li>
  
    



<a class="current" title="Archives Unleashed Toolkit" href="https://archivesunleashed.org/aut/">
	
	Archives Unleashed Toolkit
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Archives Unleashed Cloud" href="https://archivesunleashed.org/cloud/">
	
	Archives Unleashed Cloud
</a>



  
</li>



<li>
  
    



<a  title="Warclight" href="https://archivesunleashed.org/warclight/">
	
	Warclight
</a>



  
</li>



<li>
  
    



<a  title="Get Involved" href="https://archivesunleashed.org/get-involved/">
	
	Get Involved
</a>



  
</li>



<li>
  
    



<a  title="Events" href="https://archivesunleashed.org/events/">
	
	Events
</a>



  
</li>



<li>
  
    



<a  title="Washington Datathon" href="https://archivesunleashed.org/washington/">
	
	Washington Datathon
</a>



  
</li>



<li>
  
    



<a  title="News (on Medium.com)" href="https://news.archivesunleashed.org">
	
	News (on Medium.com)
</a>



  
</li>


        </ul>
        

        
        <hr>
        <span class="section">Get in touch</span>
        
        <ul>
          
          <li>
            <a href="https://twitter.com/unleasharchives" target="_blank" title="@unleasharchives on Twitter">
              @unleasharchives on Twitter
            </a>
          </li>
          

          
          <li>
            <a href="https://github.com/archivesunleashed" target="_blank" title="@archivesunleashed on GitHub">
              @archivesunleashed on GitHub
            </a>
          </li>
          

          
          <li>
            <a href="mailto:archivesunleashed@gmail.com" title="Email of archivesunleashed@gmail.com">
              Contact via email
            </a>
          </li>
          
        </ul>
        
      </div>
    </div>
  </div>
</nav>

  </div>

  <article class="article">
    <div class="wrapper">
      <h1>The Archives Unleashed Toolkit </h1>

      

<h2 id="introduction">Introduction</h2>

<p><img src="https://archivesunleashed.org/images/server.jpg" alt="Internet Archive server rack" />
<em>Internet Archive servers in San Francisco, photo by Ian Milligan.</em></p>

<p>The Archives Unleashed Toolkit is an open-source platform for managing web archives built on <a href="https://hadoop.apache.org/">Hadoop</a>. The platform provides a flexible data model for storing and managing raw content as well as metadata and extracted knowledge. Tight integration with Hadoop provides powerful tools for analytics and data processing via <a href="http://spark.apache.org/">Spark</a>.</p>

<p>Most of this documentation is built on <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">resilient distributed datasets (RDD)</a>. We are working on adding support for <a href="http://localhost:1313/aut/spark%20dataframes%20tutorial">DataFrames</a>. You can read more about this in our experimental <a href="#dataframes">DataFrames section</a>.</p>

<h2 id="getting-started">Getting Started</h2>

<h3 id="quick-start">Quick Start</h3>

<p>If you don&rsquo;t want to install all the dependencies locally, you can use <a href="https://github.com/archivesunleashed/docker-aut"><code>docker-aut</code></a>. You can run the bleeding edge version of <code>aut</code> with <code>docker run --rm -it archivesunleashed/docker-aut</code> or a specific version of <code>aut</code>, such as 0.17.0 with <code>docker run --rm -it archivesunleashed/docker-aut:0.17.0</code>. More information on using <code>docker-aut</code>, such as mounting your own data, can be found <a href="https://github.com/archivesunleashed/docker-aut#use">here</a>.</p>

<div class="admonition note">
<p class="admonition-title">Want a quick walkthrough?</p>
<p>We have a walkthrough for using AUT on sample data with Docker <a href="https://archivesunleashed.org/aut/lesson">here</a>.</p>
</div>

<h3 id="dependencies">Dependencies</h3>

<p>The Archives Unleashed Toolkit requires Java.</p>

<p>For Mac OS: You can find information on Java <a href="https://java.com/en/download/help/mac_install.xml">here</a>, or install with <a href="https://brew.sh">homebrew</a> and then:</p>

<pre><code>brew cask install java8
</code></pre>

<p>For Linux: You can install Java using apt:</p>

<pre><code>apt install openjdk-8-jdk
</code></pre>

<p>Before Spark Shell can launch, JAVA_HOME must be set. If you recieve an error that JAVA_HOME is not set, you need to point it to where Java is installed. On Linux, this might be <code>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</code> or on Mac OS it might be <code>export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_74.jdk/Contents/Home</code>.</p>

<h3 id="downloading-aut">Downloading AUT</h3>

<p>The Archives Unleashed Toolkit can be <a href="https://github.com/archivesunleashed/aut/releases/download/aut-0.17.0/aut-0.17.0-fatjar.jar">downloaded as a JAR file for easy use</a>.</p>

<p>The following bash commands will download an example ARC file, and set up a directory to work with AUT. You can also <a href="https://raw.githubusercontent.com/archivesunleashed/aut/master/src/test/resources/arc/example.arc.gz">download the example ARC file here</a>.</p>

<pre><code class="language-bash">mkdir aut
cd aut
# example arc file for testing
curl -L &quot;https://raw.githubusercontent.com/archivesunleashed/aut/master/src/test/resources/arc/example.arc.gz&quot; &gt; example.arc.gz
</code></pre>

<h3 id="installing-and-running-spark-shell">Installing and Running Spark shell</h3>

<p>Remaining in the aut directory you created above, download and unzip <a href="https://archive.apache.org/dist/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz">Spark</a> from the <a href="http://spark.apache.org/downloads.html">Apache Spark Website</a>.</p>

<pre><code class="language-bash">curl -L &quot;https://archive.apache.org/dist/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz&quot; &gt; spark-2.3.2-bin-hadoop2.7.tgz
tar -xvf spark-2.3.2-bin-hadoop2.7.tgz
./spark-2.3.2-bin-hadoop2.7/bin/spark-shell --packages &quot;io.archivesunleashed:aut:0.17.0&quot;
</code></pre>

<p>You should have the spark shell ready and running.</p>

<pre><code>Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.2
      /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_151)
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt;
</code></pre>

<blockquote>
<p>If you recently upgraded your MacOS, your java version may not be correct in terminal.  You will
have to <a href="https://stackoverflow.com/questions/21964709/how-to-set-or-change-the-default-java-jdk-version-on-os-x">change the path to the latest version in your ./bash_profile file.</a>.</p>
</blockquote>

<h3 id="test-the-archives-unleashed-toolkit">Test the Archives Unleashed Toolkit</h3>

<p>Type <code>:paste</code> at the scala prompt and go into paste mode.</p>

<p>Type or paste the following:</p>

<pre><code>import io.archivesunleashed._
import io.archivesunleashed.matchbox._

val r = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
.keepValidPages()
.map(r =&gt; ExtractDomain(r.getUrl))
.countItems()
.take(10)
</code></pre>

<p>then <code>&lt;ctrl&gt; d</code> to exit paste mode and run the script.</p>

<p>If you see:</p>

<pre><code>r: Array[(String, Int)] = Array((www.archive.org,132), (deadlists.com,2), (www.hideout.com.br,1))
</code></pre>

<p>That means you&rsquo;re up and running!</p>

<h3 id="a-note-on-memory">A Note on Memory</h3>

<p>As your datasets grow, you may need to provide more memory to Spark shell. You&rsquo;ll know this if you get an error saying that you have run out of &ldquo;Java Heap Space.&rdquo;</p>

<p>If you&rsquo;re running locally, you can pass it in your startup command like this:</p>

<pre><code>./spark-2.3.2-bin-hadoop2.7/bin/spark-shell --driver-memory 4G --packages &quot;io.archivesunleashed:aut:0.17.0&quot;
</code></pre>

<p>In the above case, you give Spark 4GB of memory to execute the program.</p>

<p>In some other cases, despite giving AUT sufficient memory, you may still encounter Java Heap Space issues. In those cases, it is worth trying to lower the number of worker threads. When running locally (i.e. on a single laptop, desktop, or server), by default AUT runs a number of threads equivalent to the number of cores in your machine.</p>

<p>On a 16-core machine, you may want to drop to 12 cores if you are having memory issues. This will increase stability but decrease performance a bit.</p>

<p>You can do so like this (example is using 12 threads on a 16-core machine):</p>

<pre><code>./spark-2.3.2-bin-hadoop2.7/bin/spark-shell --master local[12] --driver-memory 4G --packages &quot;io.archivesunleashed:aut:0.17.0&quot;
</code></pre>

<p>If you continue to have errors, you may also want to increase the network timeout value. Once in a while, AUT might get stuck on an odd record and take longer than normal to process it. The <code>--conf spark.network.timeout=10000000</code> will ensure that AUT continues to work on material, although it may take a while to process. This command then works:</p>

<pre><code>./spark-2.3.2-bin-hadoop2.7/bin/spark-shell --master local[12] --driver-memory 90G --conf spark.network.timeout=10000000 --packages &quot;io.archivesunleashed:aut:0.17.0&quot;
</code></pre>

<h2 id="collection-analytics">Collection Analytics</h2>

<p>You may want to get a birds-eye view of your ARCs or WARCs: what top-level domains are included, and at what times were they crawled?</p>

<h3 id="list-of-urls">List of URLs</h3>

<p>If you just want a list of URLs in the collection, you can type :p into Spark Shell, paste the script, and then run it with ctrl-d:</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

val r = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
.keepValidPages()
.map(r =&gt; r.getUrl)
.take(10)
</code></pre>

<p>This will give you a list of the top ten URLs. If you want all the URLs, exported to a file, you could run this instead. Note that your export directory cannot already exist.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

val r = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
.keepValidPages()
.map(r =&gt; r.getUrl)
.saveAsTextFile(&quot;/path/to/export/directory/&quot;)
</code></pre>

<h3 id="list-of-top-level-domains">List of Top-Level Domains</h3>

<p>You may just want to see the domains within an item. The script below shows the top ten domains within a given file or set of files.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

val r =
RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
.keepValidPages()
.map(r =&gt; ExtractDomain(r.getUrl))
.countItems()
.take(10)
</code></pre>

<p>If you want to see more than ten results, change the variable in the last line.</p>

<h3 id="list-of-different-subdomains">List of Different Subdomains</h3>

<p>Finally, you can use regular expressions to extract more fine-tuned information. For example, if you wanted to know all sitenames - i.e. the first-level directories of a given collection.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

val r = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
 .keepValidPages()
 .flatMap(r =&gt; &quot;&quot;&quot;http://[^/]+/[^/]+/&quot;&quot;&quot;.r.findAllIn(r.getUrl).toList)
 .take(10)
</code></pre>

<p>In the above example, <code>&quot;&quot;&quot;....&quot;&quot;&quot;</code> declares that we are working with a regular expression, <code>.r</code> says turn it into a regular expression, <code>.findAllIn</code> says look for all matches in the URL. This will only return the first but that is generally good for our use cases. Finally, <code>.toList</code> turns it into a list so you can <code>flatMap</code>.</p>

<h2 id="plain-text-extraction">Plain Text Extraction</h2>

<h3 id="all-plain-text">All plain text</h3>

<p>This script extracts the crawl date, domain, URL, and plain text from HTML files in the sample ARC data (and saves the output to out/).</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .map(r =&gt; (r.getCrawlDate, r.getDomain, r.getUrl, RemoveHTML(r.getContentString)))
  .saveAsTextFile(&quot;plain-text/&quot;)
</code></pre>

<p>If you wanted to use it on your own collection, you would change &ldquo;src/test/resources/arc/example.arc.gz&rdquo; to the directory with your own ARC or WARC files, and change &ldquo;out/&rdquo; on the last line to where you want to save your output data.</p>

<p>Note that this will create a new directory to store the output, which cannot already exist.</p>

<h3 id="plain-text-by-domain">Plain text by domain</h3>

<p>The following Spark script generates plain text renderings for all the web pages in a collection with a URL matching a filter string. In the example case, it will go through the collection and find all of the URLs within the &ldquo;archive.org&rdquo; domain.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .keepDomains(Set(&quot;www.archive.org&quot;))
  .map(r =&gt; (r.getCrawlDate, r.getDomain, r.getUrl, RemoveHTML(r.getContentString)))
  .saveAsTextFile(&quot;plain-text-domain/&quot;)
</code></pre>

<h3 id="plain-text-by-url-pattern">Plain text by URL pattern</h3>

<p>The following Spark script generates plain text renderings for all the web pages in a collection with a URL matching a regular expression pattern. In the example case, it will go through a WARC file and find all of the URLs beginning with <code>http://archive.org/details/</code>, and save the text of those URLs.</p>

<p>The <code>(?i)</code> makes this query case insensitive.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .keepUrlPatterns(Set(&quot;(?i)http://www.archive.org/details/.*&quot;.r))
  .map(r =&gt; (r.getCrawlDate, r.getDomain, r.getUrl, RemoveHTML(r.getContentString)))
  .saveAsTextFile(&quot;details/&quot;)
</code></pre>

<h3 id="plain-text-minus-boilerplate">Plain text minus boilerplate</h3>

<p>The following Spark script generates plain text renderings for all the web pages in a collection, minus &ldquo;boilerplate&rdquo; content: advertisements, navigational elements, and elements of the website template. For more on the boilerplate removal library we are using, <a href="http://www.l3s.de/~kohlschuetter/boilerplate/">please see this website and paper</a>.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .keepDomains(Set(&quot;www.archive.org&quot;))
  .map(r =&gt; (r.getCrawlDate, r.getDomain, r.getUrl, ExtractBoilerpipeText(r.getContentString)))
  .saveAsTextFile(&quot;plain-text-no-boilerplate/&quot;)
</code></pre>

<h3 id="plain-text-filtered-by-date">Plain text filtered by date</h3>

<p>AUT permits you to filter records by a list of full or partial date strings. It conceives
of the date string as a <code>DateComponent</code>. Use <code>keepDate</code> to specify the year (<code>YYYY</code>), month (<code>MM</code>),
day (<code>DD</code>), year and month (<code>YYYYMM</code>), or a particular year-month-day (<code>YYYYMMDD</code>).</p>

<p>The following Spark script extracts plain text for a given collection by date (in this case, April 2008).</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .keepDate(List(&quot;200804&quot;), ExtractDate.DateComponent.YYYYMM)
  .map(r =&gt; (r.getCrawlDate, r.getDomain, r.getUrl, RemoveHTML(r.getContentString)))
  .saveAsTextFile(&quot;plain-text-date-filtered-200804/&quot;)
</code></pre>

<p>The following script extracts plain text for a given collection by year (in this case, 2008).</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .keepDate(List(&quot;2008&quot;), ExtractDate.DateComponent.YYYY)
  .map(r =&gt; (r.getCrawlDate, r.getDomain, r.getUrl, RemoveHTML(r.getContentString)))
  .saveAsTextFile(&quot;plain-text-date-filtered-2008/&quot;)
</code></pre>

<p>Finally, you can also extract multiple dates or years. In this case, we would extract pages from both 2008 and 2015.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .keepDate(List(&quot;2008&quot;,&quot;2015&quot;), ExtractDate.DateComponent.YYYY)
  .map(r =&gt; (r.getCrawlDate, r.getDomain, r.getUrl, RemoveHTML(r.getContentString)))
  .saveAsTextFile(&quot;plain-text-date-filtered-2008-2015/&quot;)
</code></pre>

<p>Note: if you created just a dump of plain text using another one of the earlier commands, you do not need to go back and run this. You can instead use bash to extract a sample of text. For example, running this command on a dump of all plain text stored in <code>alberta_education_curriculum.txt</code>:</p>

<pre><code class="language-bash">sed -n -e '/^(201204/p' alberta_education_curriculum.txt &gt; alberta_education_curriculum-201204.txt
</code></pre>

<p>Would select just the lines beginning with <code>(201204</code>, or April 2012.</p>

<h3 id="plain-text-filtered-by-language">Plain text filtered by language</h3>

<p>The following Spark script keeps only French language pages from a certain top-level domain. It uses the <a href="https://www.loc.gov/standards/iso639-2/php/code_list.php">ISO 639.2 language codes</a>.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
.keepValidPages()
.keepDomains(Set(&quot;www.archive.org&quot;))
.keepLanguages(Set(&quot;fr&quot;))
.map(r =&gt; (r.getCrawlDate, r.getDomain, r.getUrl, RemoveHTML(r.getContentString)))
.saveAsTextFile(&quot;plain-text-fr/&quot;)
</code></pre>

<h3 id="plain-text-filtered-by-keyword">Plain text filtered by keyword</h3>

<p>The following Spark script keeps only pages containing a certain keyword, which also stacks on the other scripts.</p>

<p>For example, the following script takes all pages containing the keyword &ldquo;radio&rdquo; in a collection.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

val r = RecordLoader.loadArchives(&quot;example.arc.gz&quot;,sc)
.keepValidPages()
.keepContent(Set(&quot;radio&quot;.r))
.map(r =&gt; (r.getCrawlDate, r.getDomain, r.getUrl, RemoveHTML(r.getContentString)))
.saveAsTextFile(&quot;plain-text-radio/&quot;)
</code></pre>

<p>There is also <code>discardContent</code> which does the opposite, if you have a frequent keyword you are not interested in.</p>

<h2 id="raw-html-extraction">Raw HTML Extraction</h2>

<p>In most cases, users will be interested in working with plain text. In some cases, however, you may want to work with the acutal HTML of the pages themselves (for example, looking for specific tags or HTML content).</p>

<p>The following script will produce the raw HTML of a WARC file. You can use the filters from above to filter it down accordingly by domain, language, etc.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .map(r =&gt; (r.getCrawlDate, r.getDomain, r.getUrl, r.getContentString))
  .saveAsTextFile(&quot;plain-html/&quot;)
</code></pre>

<h2 id="named-entity-recognition">Named Entity Recognition</h2>

<div class="admonition warning">
<p class="admonition-title">NER is Extremely Resource Intensive and Time Consuming</p>
<p>Named Entity Recognition is extremely resource intensive, and will take a very long time. Our recommendation is to begin testing NER on one or two WARC files, before trying it on a larger body of information. Depending on the speed of your system, it can take a day or two to process information that you are used to working with in under an hour.</p>
</div>

<p>The following Spark scripts use the <a href="http://nlp.stanford.edu/software/CRF-NER.shtml">Stanford Named Entity Recognizer</a> to extract names of entities – persons, organizations, and locations – from collections of ARC/WARC files or extracted texts. You can find a version of Stanford NER in <a href="https://github.com/archivesunleashed/aut-resources">our aut-Resources repo located here</a>.</p>

<p>The scripts require a NER classifier model. There is one provided in the Stanford NER package (in the <code>classifiers</code> folder) called <code>english.all.3class.distsim.crf.ser.gz</code>, but you can also use your own.</p>

<h3 id="extract-entities-from-arc-warc-files">Extract entities from ARC/WARC files</h3>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.app._
import io.archivesunleashed.matchbox._

ExtractEntities.extractFromRecords(&quot;/path/to/classifier/english.all.3class.distsim.crf.ser.gz&quot;, &quot;example.arc.gz&quot;, &quot;output-ner/&quot;, sc)
</code></pre>

<p>Note the call to <code>addFile()</code>. This is necessary if you are running this script on a cluster; it puts a copy of the classifier on each worker node. The classifier and input file paths may be local or on the cluster (e.g., <code>hdfs:///user/joe/collection/</code>).</p>

<p>The output of this script and the one below will consist of lines that look like this:</p>

<pre><code>(20090204,http://greenparty.ca/fr/node/6852?size=display,{&quot;PERSON&quot;:[&quot;Parti Vert&quot;,&quot;Paul Maillet&quot;,&quot;Adam Saab&quot;],
&quot;ORGANIZATION&quot;:[&quot;GPC Candidate Ottawa Orleans&quot;,&quot;Contact Cabinet&quot;,&quot;Accueil Paul Maillet GPC Candidate Ottawa Orleans Original&quot;,&quot;Circonscriptions Nouvelles Événements Blogues Politiques Contact Mon Compte&quot;],
&quot;LOCATION&quot;:[&quot;Canada&quot;,&quot;Canada&quot;,&quot;Canada&quot;,&quot;Canada&quot;]})
</code></pre>

<p>This following script takes the plain text that you may have extracted earlier and extracts the entities.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.app._
import io.archivesunleashed.matchbox._

sc.addFile(&quot;/path/to/classifier&quot;)

ExtractEntities.extractFromScrapeText(&quot;english.all.3class.distsim.crf.ser.gz&quot;, &quot;/path/to/extracted/text&quot;, &quot;output-ner/&quot;, sc)
</code></pre>

<h2 id="analysis-of-site-link-structure">Analysis of Site Link Structure</h2>

<p>Site link structures can be very useful, allowing you to learn such things as:</p>

<ul>
<li>what websites were the most linked to;</li>
<li>what websites had the most outbound links;</li>
<li>what paths could be taken through the network to connect pages;</li>
<li>what communities existed within the link structure?</li>
</ul>

<p>Most of the following examples show the <strong>domain</strong> to <strong>domain</strong> links. For example, you discover how many times that <code>liberal.ca</code> linked to <code>twitter.com</code>, rather than learning that <code>http://liberal.ca/contact</code> linked to <code>http://twitter.com/liberal_party</code>. The reason we do that is that in general, if you are working with any data at scale, the sheer number of raw URLs can become overwhelming.</p>

<p>We do provide one example below that provides raw data, however.</p>

<h3 id="extraction-of-simple-site-link-structure">Extraction of Simple Site Link Structure</h3>

<p>If your web archive does not have a temporal component, the following Spark script will generate the site-level link structure.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._
import io.archivesunleashed.util._

val links = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .flatMap(r =&gt; ExtractLinks(r.getUrl, r.getContentString))
  .map(r =&gt; (ExtractDomain(r._1).removePrefixWWW(), ExtractDomain(r._2).removePrefixWWW()))
  .filter(r =&gt; r._1 != &quot;&quot; &amp;&amp; r._2 != &quot;&quot;)
  .countItems()
  .filter(r =&gt; r._2 &gt; 5)

links.saveAsTextFile(&quot;links-all/&quot;)
</code></pre>

<p>Note how you can add filters. In this case, we add a filter so you are looking at a network graph of pages containing the phrase &ldquo;apple.&rdquo; Filters can go immediately after <code>.keepValidPages()</code>.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._
import io.archivesunleashed.util._

val links = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .keepContent(Set(&quot;apple&quot;.r))
  .flatMap(r =&gt; ExtractLinks(r.getUrl, r.getContentString))
  .map(r =&gt; (ExtractDomain(r._1).removePrefixWWW(), ExtractDomain(r._2).removePrefixWWW()))
  .filter(r =&gt; r._1 != &quot;&quot; &amp;&amp; r._2 != &quot;&quot;)
  .countItems()
  .filter(r =&gt; r._2 &gt; 5)

links.saveAsTextFile(&quot;links-all-apple/&quot;)
</code></pre>

<h3 id="extraction-of-a-link-structure-using-raw-urls-not-domains">Extraction of a Link Structure, using Raw URLs (not domains)</h3>

<p>This following script extracts all of the hyperlink relationships between sites, using the full URL pattern.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

val links = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .flatMap(r =&gt; ExtractLinks(r.getUrl, r.getContentString))
  .filter(r =&gt; r._1 != &quot;&quot; &amp;&amp; r._2 != &quot;&quot;)
  .countItems()

links.saveAsTextFile(&quot;full-links-all/&quot;)
</code></pre>

<p>You can see that the above was achieved by removing the .map(r =&gt; (ExtractDomain(r._1).removePrefixWWW(), ExtractDomain(r._2).removePrefixWWW())) line.</p>

<p>In a larger collection, you might want to add the following line:</p>

<pre><code>.filter(r =&gt; r._2 &gt; 5)
</code></pre>

<p>Before <code>.countItems()</code> to find just the documents that are linked to more than five times. As you can imagine, raw URLs are very numerous!</p>

<h3 id="extraction-of-a-site-link-structure-organized-by-url-pattern">Extraction of a Site Link Structure, organized by URL pattern</h3>

<p>In this following example, we run the same script but only extract links coming from URLs matching the pattern <code>http://www.archive.org/details/*</code>. We do so by using the <code>keepUrlPatterns</code> command.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._
import io.archivesunleashed.util._

val links = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .keepUrlPatterns(Set(&quot;(?i)http://www.archive.org/details/.*&quot;.r))
  .flatMap(r =&gt; ExtractLinks(r.getUrl, r.getContentString))
  .map(r =&gt; (ExtractDomain(r._1).removePrefixWWW(), ExtractDomain(r._2).removePrefixWWW()))
  .filter(r =&gt; r._1 != &quot;&quot; &amp;&amp; r._2 != &quot;&quot;)
  .countItems()
  .filter(r =&gt; r._2 &gt; 5)

links.saveAsTextFile(&quot;details-links-all/&quot;)
</code></pre>

<h3 id="grouping-by-crawl-date">Grouping by Crawl Date</h3>

<p>The following Spark script generates the aggregated site-level link structure, grouped by crawl date (YYYYMMDD). It
makes use of the <code>ExtractLinks</code> and <code>ExtractToLevelDomain</code> functions.</p>

<p>If you prefer to group by crawl month (YYYMM), replace <code>getCrawlDate</code> with <code>getCrawlMonth</code> below. If you prefer to group by simply crawl year (YYYY), replace <code>getCrawlDate</code> with <code>getCrawlYear</code> below.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .map(r =&gt; (r.getCrawlDate, ExtractLinks(r.getUrl, r.getContentString)))
  .flatMap(r =&gt; r._2.map(f =&gt; (r._1, ExtractDomain(f._1).replaceAll(&quot;^\\s*www\\.&quot;, &quot;&quot;), ExtractDomain(f._2).replaceAll(&quot;^\\s*www\\.&quot;, &quot;&quot;))))
  .filter(r =&gt; r._2 != &quot;&quot; &amp;&amp; r._3 != &quot;&quot;)
  .countItems()
  .filter(r =&gt; r._2 &gt; 5)
  .saveAsTextFile(&quot;sitelinks-by-date/&quot;)
</code></pre>

<p>The format of this output is:
- Field one: Crawldate, yyyyMMdd
- Field two: Source domain (i.e. liberal.ca)
- Field three: Target domain of link (i.e. ndp.ca)
- Field four: number of links.</p>

<pre><code>((20080612,liberal.ca,liberal.ca),1832983)
((20060326,ndp.ca,ndp.ca),1801775)
((20060426,ndp.ca,ndp.ca),1771993)
((20060325,policyalternatives.ca,policyalternatives.ca),1735154)
</code></pre>

<p>In the above example, you are seeing links within the same domain.</p>

<p>Note also that <code>ExtractLinks</code> takes an optional third parameter of a base URL. If you set this – typically to the source URL –
ExtractLinks will resolve a relative path to its absolute location. For example, if
<code>val url = &quot;http://mysite.com/some/dirs/here/index.html&quot;</code> and <code>val html = &quot;... &lt;a href='../contact/'&gt;Contact&lt;/a&gt; ...&quot;</code>, and we call <code>ExtractLinks(url, html, url)</code>, the list it returns will include the
item <code>(http://mysite.com/a/b/c/index.html, http://mysite.com/a/b/contact/, Contact)</code>. It may
be useful to have this absolute URL if you intend to call <code>ExtractDomain</code> on the link
and wish it to be counted.</p>

<h3 id="exporting-as-tsv">Exporting as TSV</h3>

<p>Archive records are represented in Spark as <a href="https://en.wikipedia.org/wiki/Tuple">tuples</a>,
and this is the standard format of results produced by most of the scripts presented here
(e.g., see above). It may be useful, however, to have this data in TSV (tab-separated value)
format, for further processing outside AUT. The following script uses <code>tabDelimit</code> (from
<code>TupleFormatter</code>) to transform tuples to tab-delimited strings; it also flattens any
nested tuples. (This is the same script as at the top of the page, with the addition of the
third and the second-last lines.)</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._
import io.archivesunleashed.matchbox.TupleFormatter._

RecordLoader.loadArchives(&quot;/path/to/arc&quot;, sc)
  .keepValidPages()
  .map(r =&gt; (r.getCrawlDate, ExtractLinks(r.getUrl, r.getContentString)))
  .flatMap(r =&gt; r._2.map(f =&gt; (r._1, ExtractDomain(f._1).replaceAll(&quot;^\\s*www\\.&quot;, &quot;&quot;), ExtractDomain(f._2).replaceAll(&quot;^\\s*www\\.&quot;, &quot;&quot;))))
  .filter(r =&gt; r._2 != &quot;&quot; &amp;&amp; r._3 != &quot;&quot;)
  .countItems()
  .filter(r =&gt; r._2 &gt; 5)
  .map(tabDelimit(_))
  .saveAsTextFile(&quot;sitelinks-tsv/&quot;)
</code></pre>

<p>Its output looks like:</p>

<pre><code>20151107        liberal.ca      youtube.com     16334
20151108        socialist.ca    youtube.com     11690
20151108        socialist.ca    ustream.tv      11584
20151107        canadians.org   canadians.org   11426
20151108        canadians.org   canadians.org   11403
</code></pre>

<h3 id="filtering-by-url">Filtering by URL</h3>

<p>In this case, you would only receive links coming from websites in matching the URL pattern listed under <code>keepUrlPatterns</code>.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

val links = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .keepUrlPatterns(Set(&quot;http://www.archive.org/details/.*&quot;.r))
  .map(r =&gt; (r.getCrawlDate, ExtractLinks(r.getUrl, r.getContentString)))
  .flatMap(r =&gt; r._2.map(f =&gt; (r._1, ExtractDomain(f._1).replaceAll(&quot;^\\s*www\\.&quot;, &quot;&quot;), ExtractDomain(f._2).replaceAll(&quot;^\\s*www\\.&quot;, &quot;&quot;))))
  .filter(r =&gt; r._2 != &quot;&quot; &amp;&amp; r._3 != &quot;&quot;)
  .countItems()
  .filter(r =&gt; r._2 &gt; 5)
  .saveAsTextFile(&quot;sitelinks-details/&quot;)
</code></pre>

<h3 id="exporting-to-gephi-directly">Exporting to Gephi Directly</h3>

<p>You may want to export your data directly to the <a href="http://gephi.github.io/">Gephi software suite</a>, an open-soure network analysis project. The following code writes to the GEXF format:</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.app._
import io.archivesunleashed.matchbox._

val links = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .map(r =&gt; (r.getCrawlDate, ExtractLinks(r.getUrl, r.getContentString)))
  .flatMap(r =&gt; r._2.map(f =&gt; (r._1, ExtractDomain(f._1).replaceAll(&quot;^\\s*www\\.&quot;, &quot;&quot;), ExtractDomain(f._2).replaceAll(&quot;^\\s*www\\.&quot;, &quot;&quot;))))
  .filter(r =&gt; r._2 != &quot;&quot; &amp;&amp; r._3 != &quot;&quot;)
  .countItems()
  .filter(r =&gt; r._2 &gt; 5)

WriteGEXF(links, &quot;links-for-gephi.gexf&quot;)
</code></pre>

<p>This file can then be directly opened by Gephi.</p>

<p>We also support exporting to the GraphML format. To do so, swap <code>WriteGEXF</code> in the command above with <code>WriteGraphML</code>.</p>

<h2 id="image-analysis">Image Analysis</h2>

<p>AUT supports image analysis, a growing area of interest within web archives.</p>

<h3 id="most-frequent-image-urls-in-a-collection">Most frequent image URLs in a collection</h3>

<p>The following script:</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

val links = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .keepValidPages()
  .flatMap(r =&gt; ExtractImageLinks(r.getUrl, r.getContentString))
  .countItems()
  .take(10)
</code></pre>

<p>Will extract the top ten URLs of images found within a collection, in an array like so:</p>

<pre><code>links: Array[(String, Int)] = Array((http://www.archive.org/images/star.png,408), (http://www.archive.org/images/no_star.png,122), (http://www.archive.org/images/logo.jpg,118), (http://www.archive.org/images/main-header.jpg,84), (http://www.archive.org/images/rss.png,20), (http://www.archive.org/images/mail.gif,13), (http://www.archive.org/images/half_star.png,10), (http://www.archive.org/images/arrow.gif,7), (http://ia300142.us.archive.org/3/items/americana/am_libraries.gif?cnt=0,3), (http://ia310121.us.archive.org/2/items/GratefulDead/gratefuldead.gif?cnt=0,3), (http://www.archive.org/images/wayback.gif,2), (http://www.archive.org/images/wayback-election2000.gif,2), (http://www.archive.org/images/wayback-wt...
</code></pre>

<p>If you wanted to work with the images, you could download them from the Internet Archive.</p>

<p>Let&rsquo;s use the top-ranked example. <a href="http://web.archive.org/web/*/http://archive.org/images/star.png">This link</a>, for example, will show you the temporal distribution of the image. For a snapshot from September 2007, this URL would work:</p>

<p><a href="http://web.archive.org/web/20070913051458/http://www.archive.org/images/star.png">http://web.archive.org/web/20070913051458/http://www.archive.org/images/star.png</a></p>

<p>To do analysis on all images, you could thus prepend <code>http://web.archive.org/web/20070913051458/</code> to each URL and <code>wget</code> them en masse.</p>

<p>For more information on <code>wget</code>, please consult <a href="http://programminghistorian.org/lessons/automated-downloading-with-wget">this lesson available on the Programming Historian website</a>.</p>

<h3 id="most-frequent-images-in-a-collection-based-on-md5-hash">Most frequent images in a collection, based on MD5 hash</h3>

<p>Some images may be the same, but have different URLs. This UDF finds the popular images by calculating the MD5 hash of each and presenting the most frequent images based on that metric. This script:</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.app._
import io.archivesunleashed.matchbox._

val r = RecordLoader.loadArchives(&quot;example.arc.gz&quot;,sc).persist()
ExtractPopularImages(r, 500, sc).saveAsTextFile(&quot;500-Popular-Images&quot;)
</code></pre>

<p>Will save the 500 most popular URLs to an output directory.</p>

<h2 id="twitter-analysis">Twitter Analysis</h2>

<p>AUT also supports parsing and analysis of large volumes of Twitter JSON. This allows you to work with social media and web archiving together on one platform. We are currently in active development. If you have any suggestions or want more features, feel free to pitch in at <a href="https://github.com/archivesunleashed/aut">our AUT repository</a>.</p>

<h3 id="gathering-twitter-json-data">Gathering Twitter JSON Data</h3>

<p>To gather Twitter JSON, you will need to use the Twitter API to gather information. We recommend <a href="https://github.com/edsu/twarc">twarc</a>, a &ldquo;command line tool (and Python library) for archiving Twitter JSON.&rdquo; Nick Ruest and Ian Milligan wrote an open-access article on using twarc to archive an ongoing event, which <a href="https://github.com/web-archive-group/ELXN42-Article/blob/master/elxn42.md">you can read here</a>.</p>

<p>For example, with twarc, you could begin using the searching API (stretching back somewhere between six and nine days) on the #elxn42 hashtag with:</p>

<pre><code>twarc.py --search &quot;#elxn42&quot; &gt; elxn42-search.json
</code></pre>

<p>Or you could use the streaming API with:</p>

<pre><code>twarc.py --stream &quot;#elxn42&quot; &gt; elxn42-stream.json
</code></pre>

<p>Functionality is similar to other parts of AUT, but note that you use <code>loadTweets</code> rather than <code>loadArchives</code>.</p>

<h3 id="basic-twitter-analysis">Basic Twitter Analysis</h3>

<p>With the ensuing JSON file (or directory of JSON files), you can use the following scripts. Here we&rsquo;re using the &ldquo;top ten&rdquo;, but you can always save all of the results to a text file if you desire.</p>

<h3 id="an-example-script-annotated">An Example script, annotated</h3>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._
import io.archivesunleashed.util.TweetUtils._

// Load tweets from HDFS
val tweets = RecordLoader.loadTweets(&quot;/path/to/tweets&quot;, sc)

// Count them
tweets.count()

// Extract some fields
val r = tweets.map(tweet =&gt; (tweet.id, tweet.createdAt, tweet.username, tweet.text, tweet.lang,
                             tweet.isVerifiedUser, tweet.followerCount, tweet.friendCount))

// Take a sample of 10 on console
r.take(10)

// Count the different number of languages
val s = tweets.map(tweet =&gt; tweet.lang).countItems().collect()

// Count the number of hashtags
// (Note we don't 'collect' here because it's too much data to bring into the shell)
val hashtags = tweets.map(tweet =&gt; tweet.text)
                     .filter(text =&gt; text != null)
                     .flatMap(text =&gt; {&quot;&quot;&quot;#[^ ]+&quot;&quot;&quot;.r.findAllIn(text).toList})
                     .countItems()

// Take the top 10 hashtags
hashtags.take(10)
</code></pre>

<p>The above script does the following:</p>

<ul>
<li>loads the tweets;</li>
<li>counts them;</li>
<li>extracts specific fields based on the Twitter JSON;</li>
<li>Samples them;</li>
<li>counts languages;</li>
<li>and counts and lets you know the top 10 hashtags in a collection.</li>
</ul>

<h3 id="parsing-a-specific-field">Parsing a Specific Field</h3>

<p>For example, a user may want to parse a specific field. Here we explore the <code>created_at</code> field.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._
import io.archivesunleashed.util.TweetUtils._
import java.text.SimpleDateFormat
import java.util.TimeZone

val tweets = RecordLoader.loadTweets(&quot;/shared/uwaterloo/uroc2017/tweets-2016-11&quot;, sc)

val counts = tweets.map(tweet =&gt; tweet.createdAt)
  .mapPartitions(iter =&gt; {
      TimeZone.setDefault(TimeZone.getTimeZone(&quot;UTC&quot;))
      val dateIn = new SimpleDateFormat(&quot;EEE MMM dd HH:mm:ss ZZZZZ yyyy&quot;)
      val dateOut = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;)
    iter.map(d =&gt; try { dateOut.format(dateIn.parse(d)) } catch { case e: Exception =&gt; null })})
  .filter(d =&gt; d != null)
  .countItems()
  .sortByKey()
  .collect()
</code></pre>

<p>The next example takes the parsed <code>created_at</code> field with some of the earlier elements to see how often the user @HillaryClinton (or any other user) was mentioned in a corpus.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._
import io.archivesunleashed.util.TweetUtils._
import java.text.SimpleDateFormat
import java.util.TimeZone

val tweets = RecordLoader.loadTweets(&quot;/shared/uwaterloo/uroc2017/tweets-2016-11/&quot;, sc)

val clintonCounts = tweets
  .filter(tweet =&gt; tweet.text != null &amp;&amp; tweet.text.contains(&quot;@HillaryClinton&quot;))
  .map(tweet =&gt; tweet.createdAt)
  .mapPartitions(iter =&gt; {
      TimeZone.setDefault(TimeZone.getTimeZone(&quot;UTC&quot;))
      val dateIn = new SimpleDateFormat(&quot;EEE MMM dd HH:mm:ss ZZZZZ yyyy&quot;)
      val dateOut = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;)
    iter.map(d =&gt; try { dateOut.format(dateIn.parse(d)) } catch { case e: Exception =&gt; null })})
  .filter(d =&gt; d != null)
  .countItems()
  .sortByKey()
  .collect()
</code></pre>

<h3 id="parsing-json">Parsing JSON</h3>

<p>What if you want to do more and access more data inside tweets?
Tweets are just JSON objects, see examples
<a href="https://gist.github.com/hrp/900964">here</a> and
<a href="https://gist.github.com/gnip/764239">here</a>.  Twitter has <a href="https://dev.twitter.com/overview/api/tweets">detailed
API documentation</a> that
tells you what all the fields mean.</p>

<p>The Archives Unleashed Toolkit internally uses
<a href="https://github.com/json4s/json4s">json4s</a> to access fields in
JSON. You can manipulate fields directly to access any part of tweets.
Here are some examples:</p>

<pre><code class="language-scala">import org.json4s._
import org.json4s.jackson.JsonMethods._

val sampleTweet = &quot;&quot;&quot;  [insert tweet in JSON format here] &quot;&quot;&quot;
val json = parse(sampleTweet)
</code></pre>

<p>The you can do something like:</p>

<pre><code class="language-scala">implicit lazy val formats = org.json4s.DefaultFormats

// Extract id
(json \ &quot;id_str&quot;).extract[String]

// Extract created_at
(json \ &quot;created_at&quot;).extract[String]
</code></pre>

<h2 id="dataframes">DataFrames</h2>

<div class="admonition warning">
<p class="admonition-title">Troubleshooting Tips</p>
<p>This section is experimental and under development! If things don&rsquo;t work, or you have ideas for us, <a href="https://github.com/archivesunleashed/aut/issues/190">let us know</a>!</p>
</div>

<p>There are two main ways to use the Archives Unleashed Toolkit. The above instructions used <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">resilient distributed datasets (RDD)</a>.</p>

<p>We are currently developing support for <a href="spark dataframes tutorial">DataFrames</a>. This is still under active development, so syntax may change. We have an <a href="https://github.com/archivesunleashed/aut/issues/190">open thread</a> in our GitHub repository if you would like to add any suggestions, thoughts, or requests for this functionality.</p>

<p>You will note that right now we do not support everything in DataFrames: we do not support plain text extraction, named entity recognition, or Twitter analysis.</p>

<p>Here we provide some documentation on how to use DataFrames in AUT.</p>

<h3 id="list-of-domains">List of Domains</h3>

<p>As with the RDD implementation, the first stop is often to work with the frequency of domains appearing within a web archive. You can see the schema that you can use when working with domains by running the following script:</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.df._

val df = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .extractValidPagesDF()

df.printSchema()
</code></pre>

<p>The below script will show you the top domains within the collection.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.df._

val df = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .extractValidPagesDF()

df.select(ExtractBaseDomain($&quot;Url&quot;).as(&quot;Domain&quot;))
  .groupBy(&quot;Domain&quot;).count().orderBy(desc(&quot;count&quot;)).show()
</code></pre>

<p>Results will look like:</p>

<pre><code>+------------------+-----+
|            Domain|count|
+------------------+-----+
|   www.archive.org|  132|
|     deadlists.com|    2|
|www.hideout.com.br|    1|
+------------------+-----+
</code></pre>

<h3 id="hyperlink-network">Hyperlink Network</h3>

<p>You may want to work with DataFrames to extract hyperlink networks. You can see the schema with the following commands:</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.df._

val df = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .extractHyperlinksDF()

df.printSchema()
</code></pre>

<p>The below script will give you the source and destination for hyperlinks found within the archive.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.df._

val df = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc)
  .extractHyperlinksDF()

df.select(RemovePrefixWWW(ExtractBaseDomain($&quot;Src&quot;)).as(&quot;SrcDomain&quot;),
    RemovePrefixWWW(ExtractBaseDomain($&quot;Dest&quot;)).as(&quot;DestDomain&quot;))
  .groupBy(&quot;SrcDomain&quot;, &quot;DestDomain&quot;).count().orderBy(desc(&quot;SrcDomain&quot;)).show()
</code></pre>

<p>Results will look like:</p>

<pre><code>+-------------+--------------------+-----+
|    SrcDomain|          DestDomain|count|
+-------------+--------------------+-----+
|deadlists.com|       deadlists.com|    2|
|deadlists.com|           psilo.com|    2|
|deadlists.com|                    |    2|
|deadlists.com|         archive.org|    2|
|  archive.org|        cyberduck.ch|    1|
|  archive.org|        balnaves.com|    1|
|  archive.org|         avgeeks.com|    1|
|  archive.org|          cygwin.com|    1|
|  archive.org|      onthemedia.org|    1|
|  archive.org|ia311502.us.archi...|    2|
|  archive.org|dvdauthor.sourcef...|    1|
|  archive.org|              nw.com|    1|
|  archive.org|             gnu.org|    1|
|  archive.org|          hornig.net|    2|
|  archive.org|    webreference.com|    1|
|  archive.org|    bookmarklets.com|    2|
|  archive.org|ia340929.us.archi...|    2|
|  archive.org|            mids.org|    1|
|  archive.org|       gutenberg.org|    1|
|  archive.org|ia360602.us.archi...|    2|
+-------------+--------------------+-----+
only showing top 20 rows
</code></pre>

<h3 id="image-analysis-1">Image Analysis</h3>

<p>You can also use DataFrames to analyze images. You can see the schema for images by running the following command:</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

val df = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc).extractImageDetailsDF();
df.printSchema()
</code></pre>

<p>The following script will extract all the images, give you their dimensions, as well as unique hashes.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._

val df = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc).extractImageDetailsDF();
df.select($&quot;url&quot;, $&quot;mime_type&quot;, $&quot;width&quot;, $&quot;height&quot;, $&quot;md5&quot;, $&quot;bytes&quot;).orderBy(desc(&quot;md5&quot;)).show()
</code></pre>

<p>The results will look like this:</p>

<pre><code>+--------------------+----------+-----+------+--------------------+--------------------+
|                 url| mime_type|width|height|                 md5|               bytes|
+--------------------+----------+-----+------+--------------------+--------------------+
|http://www.archiv...| image/gif|   21|    21|ff05f9b408519079c...|R0lGODlhFQAVAKUpA...|
|http://www.archiv...|image/jpeg|  275|   300|fbf1aec668101b960...|/9j/4AAQSkZJRgABA...|
|http://www.archiv...|image/jpeg|  300|   225|f611b554b9a44757d...|/9j/4RpBRXhpZgAAT...|
|http://tsunami.ar...|image/jpeg|  384|   229|f02005e29ffb485ca...|/9j/4AAQSkZJRgABA...|
|http://www.archiv...| image/gif|  301|    47|eecc909992272ce0d...|R0lGODlhLQEvAPcAA...|
|http://www.archiv...| image/gif|  140|    37|e7166743861126e51...|R0lGODlhjAAlANUwA...|
|http://www.archiv...| image/png|   14|    12|e1e101f116d9f8251...|iVBORw0KGgoAAAANS...|
|http://www.archiv...|image/jpeg|  300|   116|e1da27028b81db60e...|/9j/4AAQSkZJRgABA...|
|http://www.archiv...|image/jpeg|   84|    72|d39cce8b2f3aaa783...|/9j/4AAQSkZJRgABA...|
|http://www.archiv...| image/gif|   13|    11|c7ee6d7c17045495e...|R0lGODlhDQALALMAA...|
|http://www.archiv...| image/png|   20|    15|c1905fb5f16232525...|iVBORw0KGgoAAAANS...|
|http://www.archiv...| image/gif|   35|    35|c15ec074d95fe7e1e...|R0lGODlhIwAjANUAA...|
|http://www.archiv...| image/png|  320|   240|b148d9544a1a65ae4...|iVBORw0KGgoAAAANS...|
|http://www.archiv...| image/gif|    8|    11|a820ac93e2a000c9d...|R0lGODlhCAALAJECA...|
|http://www.archiv...| image/gif|  385|    30|9f70e6cc21ac55878...|R0lGODlhgQEeALMPA...|
|http://www.archiv...|image/jpeg|  140|   171|9ed163df5065418db...|/9j/4AAQSkZJRgABA...|
|http://www.archiv...|image/jpeg| 1800|    89|9e41e4d6bdd53cd9d...|/9j/4AAQSkZJRgABA...|
|http://www.archiv...| image/gif|  304|    36|9da73cf504be0eb70...|R0lGODlhMAEkAOYAA...|
|http://www.archiv...|image/jpeg|  215|    71|97ebd3441323f9b5d...|/9j/4AAQSkZJRgABA...|
|http://i.creative...| image/png|   88|    31|9772d34b683f8af83...|iVBORw0KGgoAAAANS...|
+--------------------+----------+-----+------+--------------------+--------------------+
only showing top 20 rows
</code></pre>

<p>You may want to save the images to work with them on your own file system. The following command will save the images from an ARC or WARC.</p>

<pre><code class="language-scala">import io.archivesunleashed._
import io.archivesunleashed.matchbox._
val df = RecordLoader.loadArchives(&quot;example.arc.gz&quot;, sc).extractImageDetailsDF();
val res = df.select($&quot;bytes&quot;).orderBy(desc(&quot;bytes&quot;)).saveToDisk(&quot;bytes&quot;, &quot;/path/to/export/directory/&quot;)
</code></pre>


      <aside class="copyright" role="note">
        
        CC-BY 2018 &vert;
        
        <a href="https://github.com/archivesunleashed/archivesunleashed.org" target="_blank">Site</a> built with
        <a href="https://www.gohugo.io" target="_blank">Hugo</a>
        using the
        <a href="http://github.com/digitalcraftsman/hugo-material-docs" target="_blank">Material</a> theme
      </aside>

      <footer class="footer">
        

<nav class="pagination" aria-label="Footer">
  <div class="previous">
  
      <a href="https://archivesunleashed.org/cloud/" title="The Archives Unleashed Cloud">
        <span class="direction">
          Previous
        </span>
        <div class="page">
          <div class="button button-previous" role="button" aria-label="Previous">
            <i class="icon icon-back"></i>
          </div>
          <div class="stretch">
            <div class="title">
              The Archives Unleashed Cloud
            </div>
          </div>
        </div>
      </a>
  
  </div>

  <div class="next">
  
      <a href="https://archivesunleashed.org/warclight/" title="Warclight">
        <span class="direction">
          Next
        </span>
        <div class="page">
          <div class="stretch">
            <div class="title">
              Warclight
            </div>
          </div>
          <div class="button button-next" role="button" aria-label="Next">
            <i class="icon icon-forward"></i>
          </div>
        </div>
      </a>
  
  </div>
</nav>





      </footer>
    </div>
  </article>

  <div class="results" role="status" aria-live="polite">
    <div class="scrollable">
      <div class="wrapper">
        <div class="meta"></div>
        <div class="list"></div>
      </div>
    </div>
  </div>
</main>

    <script>
    
      var base_url = '';
      var repo_id  = '';
    
    </script>

    <script src="https://archivesunleashed.org/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    
      <script>
        (function(i,s,o,g,r,a,m){
          i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||
          []).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;
          m.parentNode.insertBefore(a,m)
        })(window, document,
          'script', '//www.google-analytics.com/analytics.js', 'ga');
         
        ga('create', 'UA-2879197-28', 'auto');
        ga('set', 'anonymizeIp', true);
        ga('send', 'pageview');
         
        var buttons = document.querySelectorAll('a');
        Array.prototype.map.call(buttons, function(item) {
          if (item.host != document.location.host) {
            item.addEventListener('click', function() {
              var action = item.getAttribute('data-action') || 'follow';
              ga('send', 'event', 'outbound', action, item.href);
            });
          }
        });
         
        var query = document.querySelector('.query');
        query.addEventListener('blur', function() {
          if (this.value) {
            var path = document.location.pathname;
            ga('send', 'pageview', path + '?q=' + this.value);
          }
        });
      </script>
    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

